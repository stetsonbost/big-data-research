# Research Log

This is a log of my progress for my big data research with Professor Weiqing Gu at Harvey Mudd College during Summer 2017. The format of these log entries may change over time as I settle on a comforable style for them.

## Tuesday, May 16
For this research, it is essential that I (re)learn core concepts of big data. As such, I am spending the first part of the research studying these. 

Today I set up my account on the Pittsburgh Supercomputing Center (PSC) server for the XSEDE big data workshop that is taking place on Thursday and Friday. I also looked at the main slides they will be using on Thursday ([Intro to Big Data](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/Intro_To_Big_Data.pdf), [Hadoop](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/BigData_Hadoop_110116.pdf), and [Spark](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/BigData_Hadoop_110116.pdf)). While these gave me a good idea of what would be covered in the workshop, it's obvious that it will be a very hands-on workshop that will teach me how to use popular computational tools like Hadoop and Spark for big data analytics.

In addition to preparing for the workshop, I also started studying material from Andrew Ng's course [CS 229: Machine Learning](http://cs229.stanford.edu/materials.html), taught at Stanford. I am beginning with his [first set of notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). It starts with an introduction to supervised learning, followed by a large section on linear regression. One thing he does is introduce the least-squares cost function, although I think several other cost functions could also be effective for linear regression. Ng talks about the least means squares (LMS) (aka Widrow-Hoff) learning rule, which makes updates to the weights of feature based on the current weight, error term, and the learning rate. An algorithm that does this is *batch gradient descent*, which works best if there is only one global optimum and no local optima. Another algorithm, *stochastic (incremental) gradient descent*, makes more frequent updates to the weights and can get to an optimum faster than batch gradient descent, making it better for larger training sets. The next part is more about how to deal with matrices and vectors, including least squares with matrices. This leads up to the normal equation, which will be very important.

Tomorrow I'll continue through these notes. The next section is on probabilistic approaches, which should line up well with where Prof. Gu will be in her big data lecture tomorrow evening.

## Wednesday, May 17

Today I continued through Ng's [first set of notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). I followed the end of the Section 2 ("The normal equations") for the derivation of the normal equation, which used many properties of matrices and gradients. I also read Section 3 ("Probabilistic interpretation"), which derived the least squares cost function by assuming that the error is normally distributed, which convinces me that least squares is probably a very good cost function for regression in general. Section 4 ("Locally weighted linear regression") started with a short discussion on the importance of a good fit (not overfitting or underfitting) and of choosing a good set of features for learning. It then talked about the locally weighted linear regression algorithm, which is similar to normal linear regression except that there are weights (w, not θ) for each term in the least squares cost function. The next big part of Ng's notes are about classification and logistic regression, where we try to fit the data to a logistic (sigmoid) function for better classification. 

I also attended Prof. Gu's big data lecture this evening. The first portion of the class was focused on using probability. She talked about the Multivariate Gaussian or normal (MVN) distribution and how to use probability for linear regression. The principal of maximal likelihood was used to choose the weight vector θ that maximizes the likelihood L(θ) or equivalently the log likelihood l(θ) = log L(θ). Then she covered the LMS algorithm, batch gradient descent (BGD), and stochastic gradient descent (SGD). For classification problems, logistic regression is often better than linear regression, once again using the techniques of maximizing log likelihood and _gradient **ascent**_. Newton's method can also be used, and it is quite fast computationally. Then she started talking about generalized linear models (GLMs), introducing the exponential family of distributions, which includes Bernoulli and Gaussian distributions.