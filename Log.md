# Research Log

This is a log of my progress for my big data research with Professor Weiqing Gu at Harvey Mudd College during Summer 2017. The format of these log entries may change over time as I settle on a comforable style for them.

## Tuesday, May 16

For this research, it is essential that I (re)learn core concepts of big data. As such, I am spending the first part of the research studying these. 

Today I set up my account on the Pittsburgh Supercomputing Center (PSC) server for the XSEDE big data workshop that is taking place on Thursday and Friday. I also looked at the main slides they will be using on Thursday ([Intro to Big Data](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/Intro_To_Big_Data.pdf), [Hadoop](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/BigData_Hadoop_110116.pdf), and [Spark](https://www.psc.edu/images/xsedetraining/BigDataFebruary2017/BigData_Hadoop_110116.pdf)). While these gave me a good idea of what would be covered in the workshop, it's obvious that it will be a very hands-on workshop that will teach me how to use popular computational tools like Hadoop and Spark for big data analytics.

In addition to preparing for the workshop, I also started studying material from Andrew Ng's course [CS 229: Machine Learning](http://cs229.stanford.edu/materials.html), taught at Stanford. I am beginning with his [first set of notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). It starts with an introduction to supervised learning, followed by a large section on linear regression. One thing he does is introduce the least-squares cost function, although I think several other cost functions could also be effective for linear regression. Ng talks about the least means squares (LMS) (aka Widrow-Hoff) learning rule, which makes updates to the weights of feature based on the current weight, error term, and the learning rate. An algorithm that does this is *batch gradient descent*, which works best if there is only one global optimum and no local optima. Another algorithm, *stochastic (incremental) gradient descent*, makes more frequent updates to the weights and can get to an optimum faster than batch gradient descent, making it better for larger training sets. The next part is more about how to deal with matrices and vectors, including least squares with matrices. This leads up to the normal equation, which will be very important.

Tomorrow I'll continue through these notes. The next section is on probabilistic approaches, which should line up well with where Prof. Gu will be in her big data lecture tomorrow evening.

## Wednesday, May 17

Today I continued through Ng's [first set of notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). I followed the end of the Section 2 ("The normal equations") for the derivation of the normal equation, which used many properties of matrices and gradients. I also read Section 3 ("Probabilistic interpretation"), which derived the least squares cost function by assuming that the error is normally distributed, which convinces me that least squares is probably a very good cost function for regression in general. Section 4 ("Locally weighted linear regression") started with a short discussion on the importance of a good fit (not overfitting or underfitting) and of choosing a good set of features for learning. It then talked about the locally weighted linear regression algorithm, which is similar to normal linear regression except that there are weights (w, not θ) for each term in the least squares cost function. The next big part of Ng's notes are about classification and logistic regression, where we try to fit the data to a logistic (sigmoid) function for better classification. 

I also attended Prof. Gu's big data lecture this evening. The first portion of the class was focused on using probability. She talked about the Multivariate Gaussian or normal (MVN) distribution and how to use probability for linear regression. The principle of maximal likelihood was used to choose the weight vector θ that maximizes the likelihood L(θ) or equivalently the log likelihood l(θ) = log L(θ). Then she covered the LMS algorithm, batch gradient descent (BGD), and stochastic gradient descent (SGD). For classification problems, logistic regression is often better than linear regression, once again using the techniques of maximizing log likelihood and _gradient ascent_. Newton's method can also be used, and it is quite fast computationally. Then she started talking about generalized linear models (GLMs), introducing the exponential family of distributions, which includes Bernoulli and Gaussian distributions.

## Thursday, May 18

The XSEDE big data workshop was pretty informative. One of the speakers talked about Hadoop, and how it works. It is based on the [Map-Reduce](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) model, where a big problem is split up (parallelized). He did some examples of doing word counts in Shakespeare's works, but this part wasn't very hands-on.

The next (main) speaker talked about Spark, which is newer than Hadoop and has more capabilities than Hadoop. We were using PySpark since Python is user-friendly and is *very* common for Spark. Spark uses Resilient Distributed Datasets (RDDs) to organized data, and has many ways to manipulate and analyze the data. We saw how to use k-means (a built-in package) in Spark to cluster the data. We then did some Spark exercises to analyze the works of Shakespeare.

At the end, the speaker talked about Bridges, the main supercomputing cluster at PSC that we were using for the workshop. Note that because of this workshop, I now have access to Bridges.

## Friday, May 19

Today was the second day of the XSEDE big data workshop. We first focused on recommender systems with the Netflix movie recommendation problem in mind. The approach was a lossy compression for an approximate solution, where we take the product of two smaller matrices to fill in rating values in a sparce matrix. The larger the matrices, the more accurate the approximation of the rating matrix will be; the best width/height (a *hyperparameter*) of the smaller matrices can be found experimentally, and if it is too large, we run the risk of overfitting or having it be too computationally expensive. We weren't using singular value decomposition because we are dealing with very large, sparce martrices, and SVD does not deal with this well. Instead we were using the (built-in) Alternating Least Squares (ALS) algorithm. PySpark seems to have many libraries and packages with machine learning algorithms.

The second part of the workshop today was about deep learning and neural networks (NNs). He talked about how NNs are organized and how they work, as well as a taste of how to determine the proper weights in a NN using backpropogation. Stochastic gradient descent is a good, fast way of doing this. TensorFlow was used for Softmax Regression on the MNIST handwritten digits dataset. He covered how convolutions and pooling work which are used for hidden layers in a NN, as well as ways to avoid overfitting.

## Saturday, May 27

This week I finished going through Ng's [first set of notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). Specifically I looked at Parts II ("Classification and logistic regression") and III ("Generalized Linear Models"). The sections in this part solidified what was taught in the last lecture I attended (on Wednesday, May 17) before I left Claremont.

Logistic regression is a good method for binary classification of data in large part due to the shape of the logistic (sigmoid) function. As with several other approaches, we can use the principle of maximum likelihood as we maximize the log likelihood using stochastic gradient ascent (SGA). We end up with the same rule for SGA as we did for LMS, but now we have a rule defined for a non-linear function. The notes briefly mention that we can get the perceptron learning algorithm by using a specific hypothesis function in the logistic regressin SGA rule, even though perceptron learning is actually very different. The next thing covered was Newton's method, as well as the Newton-Raphson method, a multidimensional version of Newton's method. These methods often converge faster than batch gradient descent. However, finding and inverting an n-by-n Hessian matrix could make Newton-Raphson slower than BGD.

The exponential family includes many common distributions, such as Bernoulli, Gaussian, Poisson, multinomial, among others. All these distributions have a natural (canonical) parameter, a sufficient statistic, a log partition function, which are related in a specific way. I worked through the examples in the notes for the Bernoulli, Gaussian, and multinomial distributions. The notes then talked about Generalized Linear Models (GLMs) with the assumptions that we have an exponential family distribution, that the hypothesis outputs the expected value, and that the natual parameter and inputs are related linearly. The GLM can be applied to a number of different models including ordinary least squares, logistic regression, and softmax regression, which deals with classification into more than than two classes.